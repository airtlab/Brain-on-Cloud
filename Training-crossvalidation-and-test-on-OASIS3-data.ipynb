{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"1LQd_dXgdj9fa2TfhBhT9nl0pM_XXHHJK","authorship_tag":"ABX9TyPKjPlj3yzwnJxQys20PjWu"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"MPkhpot7iQsD"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"STXZoZpTwhDe"},"source":["import os\n","import numpy as np\n","import cv2\n","import tensorflow as tf\n","import sklearn\n","import matplotlib.pylab as plt\n","\n","from tensorflow.keras import layers, models\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, classification_report"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Experiments:\n","\n","- **1st experiment**: OASIS-3 data with only normalization and automated crop (no registration, no brain extraction, no augmentation);\n","\n","- **2nd experiment**: OASIS-3 data with normalization, automated crop and registration w.r.t. AD/CN avg template (no brain extraction, no augmentation);\n","\n","- **3rd experiment**: OASIS-3 data with normalization, automated crop, registration w.r.t. AD/CN avg template and brain extraction (no augmentation);\n","\n","- **4th experiment**: OASIS-3 data with normalization, automated crop, registration w.r.t. AD/CN avg template and brain extraction (augmentation is here performed on the training set);\n","\n","- **Tuned best experiment (4th)**: as follows."],"metadata":{"id":"r8SLRHfirRtL"}},{"cell_type":"code","metadata":{"id":"yy6cjt_QFSRs"},"source":["path = \"...\" # insert your own path of OASIS-3 data with normalization, automated crop, registration w.r.t. AD/CN avg template and brain extraction applied (augmentation is then performed on the training set)\n","\n","X = [] \n","Y = [] \n","\n","dir = path \n","classes = [\"AD\",\"CN\"] \n","classes_list = os.listdir(dir) \n","classes_list.sort() \n","\n","full_path_AD = os.path.join(dir, classes_list[0]) \n","samples_AD = os.listdir(full_path_AD) \n","samples_AD.sort() \n","for a in samples_AD: \n","  b = os.path.join(full_path_AD, a) \n","  data = np.load(b)\n","  data = data.astype('float32') \n","  temp = [] \n","  for i in range(85,135): \n","    temp.append(cv2.resize(data[:,:,i], (192,147))) \n","  X.append(temp) \n","  y = [0]*len(classes_list) \n","  y[1] = 1 \n","  Y.append(y) \n","\n","full_path_CN = os.path.join(dir, classes_list[1]) \n","samples_CN = os.listdir(full_path_CN) \n","samples_CN.sort()\n","for a in samples_CN: \n","  b = os.path.join(full_path_CN, a)\n","  data = np.load(b)\n","  data = data.astype('float32')\n","  temp = []\n","  for i in range(85,135): \n","    temp.append(cv2.resize(data[:,:,i], (192,147)))\n","  X.append(temp)\n","  y = [0]*len(classes_list)\n","  y[0] = 1 \n","  Y.append(y) \n","  \n","X = np.asarray(X) \n","X = X[...,np.newaxis] \n","Y = np.asarray(Y) \n","\n","print(X.shape)\n","print(Y.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x5mhhsX4yxik"},"source":["model = models.Sequential()\n","model.add(layers.ConvLSTM2D(filters=8, \n","                            kernel_size=(3,3), \n","                            return_sequences=False, \n","                            data_format=\"channels_last\", \n","                            input_shape=(50, 147, 192, 1)))\n","model.add(layers.Dropout(0.6)) \n","model.add(layers.Flatten())\n","model.add(layers.Dense(256, activation=\"relu\"))\n","model.add(layers.Dropout(0.6)) \n","model.add(layers.Dense(2, activation=\"softmax\"))\n","\n","model.summary()\n","\n","opt = tf.keras.optimizers.SGD(learning_rate=0.005) \n","model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[\"accuracy\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nUDhKV17gq1M"},"source":["seed = 0\n","n_splits = 5\n","batch_size = 6\n","\n","cv = StratifiedShuffleSplit(n_splits=n_splits, train_size=0.8, random_state=seed) \n","\n","scores = []\n","loss_per_split = []\n","acc_per_split = []\n","tprs = []\n","aucs = []\n","senss = np.zeros(shape=(n_splits))\n","specs = np.zeros(shape=(n_splits))\n","f1_scores = np.zeros(shape=(n_splits))\n","mean_fpr = np.linspace(0,1,100)\n","plt.figure(num=1, figsize=(10,10))\n","\n","i = 1\n","for train, test in cv.split(X, Y):\n","\n","  X_train, X_val, Y_train, Y_val = train_test_split(X[train], Y[train], test_size=0.2,\n","                                                  shuffle=True, random_state=seed)\n","  \n","  dim = (50, 147, 192)\n","  X_train_aug = np.empty((352, *dim, 1)) \n","  Y_train_aug = np.empty((352, 2))\n","\n","  for j in range (0, 176): \n","    X_train_aug [j*2,:,:,:,:] = X_train[j,:,:,:,:]\n","    X_train_aug [(j*2)+1,:,:,:,:] = np.flip(X_train[j,:,:,:,:], 1) \n","    Y_train_aug [j*2,:] = Y_train[j,:]\n","    Y_train_aug [(j*2)+1,:] = Y_train[j,:]\n","\n","  print('-------------------------------------------------------------------------------------------------------------------------------')\n","\n","  print(f'Training and validation for split {i}:')\n","  earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=5, verbose=1, restore_best_weights=True)\n","  callbacks = [earlystop]\n","  history = model.fit(x=X_train_aug, y=Y_train_aug, validation_data=(X_val,Y_val), epochs=50, batch_size=batch_size, verbose=1, callbacks=callbacks) \n","  \n","  print(\"Evaluation on test data...\")\n","  evaluation = model.evaluate(X[test], Y[test])\n","  scores.append(evaluation)\n","  print(f'Scores for split {i}: {model.metrics_names[0]} of {evaluation[0]}; {model.metrics_names[1]} of {evaluation[1]*100}%')\n","  loss_per_split.append(evaluation[0]) \n","  acc_per_split.append(evaluation[1] * 100)\n","\n","  print('Prediction on test data...')\n","  pred = model.predict(X[test])\n","\n","  y_true = Y[test].argmax(axis=-1)\n","  y_pred = pred.argmax(axis=-1)\n","  report = classification_report(y_true, y_pred, target_names=['CN','AD'], output_dict=True)\n","  senss[i - 1] = report['AD']['recall']\n","  specs[i - 1] = report['CN']['recall']\n","  f1_scores[i - 1] = report['AD']['f1-score']\n","  print(classification_report(y_true, y_pred, target_names=['CN','AD']))\n","  print('Confusion matrix for split ' + str(i) + ':')\n","  print(confusion_matrix(y_true, y_pred))\n","\n","  fpr, tpr, thresholds = roc_curve(y_true, pred[:,1], pos_label=1)\n","  tprs.append(np.interp(mean_fpr, fpr, tpr))\n","  roc_auc = auc(fpr, tpr)\n","  aucs.append(roc_auc)\n","  plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC of split %d (AUC = %0.4f)' % (i, roc_auc))\n","\n","  i += 1\n","\n","print('%%%%%%%%% Summary of results on test data %%%%%%%%%')\n","\n","np_scores = np.array(scores)\n","losses = np_scores[:, 0:1]\n","accuracies = np_scores[:, 1:2]\n","print('Losses:')\n","print(losses)\n","print('Accuracies:')\n","print(accuracies)\n","print('Sensitivities:')\n","print(senss)\n","print('Specificities:')\n","print(specs)\n","print('F1-scores:')\n","print(f1_scores)\n","print(\"Avg loss: {0} +/- {1}\".format(np.mean(losses), np.std(losses)))\n","print(\"Avg accuracy: {0} +/- {1}\".format(np.mean(accuracies), np.std(accuracies)))\n","print(\"Avg sensitivity: {0} +/- {1}\".format(np.mean(senss), np.std(senss)))\n","print(\"Avg specificity: {0} +/- {1}\".format(np.mean(specs), np.std(specs)))\n","print(\"Avg f1-score: {0} +/- {1}\".format(np.mean(f1_scores), np.std(f1_scores)))\n","\n","plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)\n","\n","mean_tpr = np.mean(tprs, axis=0)\n","mean_auc = auc(mean_fpr, mean_tpr)\n","std_auc = np.std(aucs)\n","plt.plot(mean_fpr, mean_tpr, color='b', label='Mean ROC (AUC = %0.4f $\\pm$ %0.4f)' % (mean_auc, std_auc), lw=2, alpha=.8)\n","\n","std_tpr = np.std(tprs, axis=0)\n","tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n","tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n","plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2, label=r'$\\pm$ 1 std. dev.')\n","\n","plt.xlim([-0.01, 1.01])\n","plt.ylim([-0.01, 1.01])\n","plt.xlabel('False positive rate (1-specificity)', fontsize=18)\n","plt.ylabel('True positive rate (sensitivity)', fontsize=18)\n","plt.title('ROC', fontsize=18)\n","plt.legend(loc=\"lower right\", prop={'size': 15})\n","\n","plt.show()\n","\n","model.save(\".../model-best.h5\") # choose your own path to store the model weights that led to the lowest validation loss, to be used to evaluate the model performance on independent ADNI-2 data"],"execution_count":null,"outputs":[]}]}